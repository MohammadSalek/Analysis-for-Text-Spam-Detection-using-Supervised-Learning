{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fd414e",
   "metadata": {},
   "source": [
    "# Analysis for Text Spam Detection using Supervised Learning\n",
    "\n",
    "An NLP analysis project for text spam detection using Scikit-Learn and Pandas.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Home](#Text-Spam-Detection-using-Supervised-Learning)\n",
    "- [Description](#Description)\n",
    "- [Installation and Usage](#Installation-and-Usage)\n",
    "- [Tools and Algorithms](#Tools-and-Algorithms)\n",
    "- [Credits](#Credits)\n",
    "- First Model:\n",
    "    - [Import Modules and Files](#Import-Modules-and-Files)\n",
    "    - [Clean Dataset](#Clean-Dataset)\n",
    "    - [Standardize Labels](#Standardize-Labels)\n",
    "    - [Split Train and Test Datasets](#Split-Train-and-Test-Datasets)\n",
    "    - [Train Model](#Train-Model)\n",
    "    - [Test Results](#Test-Results)\n",
    "- Second Model:\n",
    "    - [Second Dataset Process](#Second-Dataset-Process)\n",
    "- [Analysis](#Analysis)\n",
    "    - [Overall Accuracy](#Overall-Accuracy)\n",
    "    - [Models Against Each Other](#Models-Against-Each-Other)\n",
    "    - [Email vs SMS Spam Detection](#Email-vs-SMS-Spam-Detection)\n",
    "    - [Third Dataset Process](#Third-Dataset-Process)\n",
    "    - [Two Models Against Each Other](#Two-Models-Against-Each-Other)\n",
    "    \n",
    "- [References](#References)\n",
    "\n",
    "\n",
    "## Description\n",
    "\n",
    "In this project we use a few labeled email/sms datasets, clean our datasets, split them into train and test sets, train and fit the text classification model and finally get prediction results and analyze it.\n",
    "\n",
    "## Installation and Usage\n",
    "\n",
    "Set your environment and install dependencies from [here](install/README.md).\n",
    "\n",
    "## Tools and Algorithms\n",
    "\n",
    "We used Supervised Machine Learning to train and test our created models. For model creation we used LinearSVC model from sklearn module (read more [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)) which was selected to be our main model after trying out several different other models from sklearn models. We also used sklearn pipline object to connect the streams of input and output of each job to speed up the whole training execution process. We used TfidfVectorizer object (read more [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)) to extract features from raw-text and convert them into TF-IDF features to pass to the next job in pipeline - which is LinearSVC job. Pandas module was used for working with our datasets and converting them into DataFrame objects. Sklearn metrics was used for generating reports and printing accuracy scores of the models against the tests. Markdown features is used for prettifying the obtained results.\n",
    "    \n",
    "## Credits\n",
    "\n",
    "Developed by:\n",
    "\n",
    "- [Mohammad Salek](mailto:salek.mohmd@gmail.com)\n",
    "\n",
    "Published on: 2021/07/21."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66719fa",
   "metadata": {},
   "source": [
    "## Import Modules and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a73155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules:\n",
    "from IPython.display import Markdown, display\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# Disable jedi for fast jupyter suggestions:\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "\n",
    "# prettifier function(s):\n",
    "def printmd(s):\n",
    "    display(Markdown(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59bc64d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>martin a posted tassos papadopoulos the greek ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man threatens explosion in moscow thursday aug...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klez the virus that won t die already the most...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in adding cream to spaghetti carbonara which ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0   date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...   ham\n",
       "1  martin a posted tassos papadopoulos the greek ...   ham\n",
       "2  man threatens explosion in moscow thursday aug...   ham\n",
       "3  klez the virus that won t die already the most...   ham\n",
       "4   in adding cream to spaghetti carbonara which ...   ham"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load email.csv files and view the top fields:\n",
    "def load_df(file):\n",
    "    df = pd.read_csv(file, usecols=['text','label'])\n",
    "    df.head(n=10)\n",
    "    return df\n",
    "\n",
    "\n",
    "file = \"dataset/first.csv\"\n",
    "df = load_df(file)\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e26dc",
   "metadata": {},
   "source": [
    "## Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb6763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 bad text and 0 bad label fields. Now deleting them...\n",
      "Successfully cleaned dataframe!\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values:\n",
    "def clean_dataset(df, label1, label2):\n",
    "    missing_label1_count = df.isnull().sum()[label1]\n",
    "    missing_label2_count= df.isnull().sum()[label2]\n",
    "    if missing_label1_count > 0 or missing_label2_count > 0:\n",
    "        # Find them:\n",
    "        blank_idx = []\n",
    "        for idx, label1_var, label2_var in list(df.itertuples())[:3]:\n",
    "            if (type(label1_var) == str and label1_var.isspace()) or (type(label2_var) == str and label2_var.isspace()):\n",
    "                blank_idx.append(idx)\n",
    "        # Clean them:\n",
    "        print(f\"Found {missing_label1_count} bad {label1} and {missing_label2_count} bad {label2} fields. Now deleting them...\")\n",
    "        df.dropna(inplace=True)\n",
    "        # Re-check the dataset:\n",
    "        missing_label1_count = df.isnull().sum()[label1]\n",
    "        missing_label2_count= df.isnull().sum()[label2]\n",
    "        if missing_label1_count or missing_label2_count:\n",
    "            raise Exception(\"Could not delete the fields in dataframe.\")\n",
    "        else:\n",
    "            print(\"Successfully cleaned dataframe!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = clean_dataset(df, label1=\"text\", label2=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e7c56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique labels: ['ham' 'spam']\n",
      "ham     2500\n",
      "spam     499\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check label type:\n",
    "print(\"unique labels:\", df[\"label\"].unique())\n",
    "\n",
    "# Check dataset balance:\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10195fc6",
   "metadata": {},
   "source": [
    "## Standardize Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "800ee6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to specific format:\n",
    "# This function was used to label 0/1 labels to ham/spam labels accordingly.\n",
    "def convert_binary_label_to_ham_spam(file):\n",
    "    csv_lines = None\n",
    "    with open(file, \"r\") as f:\n",
    "        csv_lines = f.readlines()\n",
    "    for idx, line in enumerate(csv_lines):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        splitted_line = line.replace(\"\\n\", \"\").split(',')\n",
    "        if splitted_line[1] == \"0\":\n",
    "            splitted_line[1] = \"ham\"\n",
    "        elif splitted_line[1] == \"1\":\n",
    "            splitted_line[1] = \"spam\"\n",
    "        new_line = \",\".join(splitted_line) + \"\\n\"    \n",
    "        if new_line != line:\n",
    "            csv_lines[idx] = new_line\n",
    "\n",
    "    with open(file, \"w\") as f:\n",
    "        f.writelines(csv_lines)\n",
    "\n",
    "\n",
    "# convert_binary_label_to_ham_spam(\"dataset/email.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fa704",
   "metadata": {},
   "source": [
    "## Split Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f146522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets: (Ratio of X_test to overall is set by test_size param)\n",
    "def train_split(df, label1, label2, test_size=0.25, random_state=7):\n",
    "    X = df[label1]\n",
    "    y = df[label2]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_split(df, \"text\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c73e090",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3d19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier object:\n",
    "def create_clf(X_train, X_test, y_train, y_test):\n",
    "    # Build pipeline:\n",
    "    text_clf = Pipeline([(\"tfidf\", TfidfVectorizer()),\n",
    "                        (\"clf\", LinearSVC())])\n",
    "    # Fit the data:\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    # Form prediction set:\n",
    "    predictions = text_clf.predict(X_test)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "predictions = create_clf(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeec57a",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241aa1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Confusion Matrix:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[612   2]\n",
      " [  9 127]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Classification Report:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99       614\n",
      "        spam       0.98      0.93      0.96       136\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.97      0.97       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span>**Overall Accuracy:**</span><br><span style='color:red'>**98.533%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print results:\n",
    "def print_results(y_test, predictions):\n",
    "    printmd(\"**Confusion Matrix:**\")\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    printmd(\"\\n**Classification Report:**\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    printmd(f\"<span>**Overall Accuracy:**</span><br><span style='color:red'>**{accuracy_score(y_test, predictions) * 100:.3f}%**</span>\")\n",
    "\n",
    "    \n",
    "# Save result:\n",
    "accuracy = {\"first\": accuracy_score(y_test, predictions) * 100}\n",
    "print_results(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5fa298",
   "metadata": {},
   "source": [
    "## Second Dataset Process\n",
    "\n",
    "The process is the same as it was for first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "383bd684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Confusion Matrix:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[908  15]\n",
      " [  5 365]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Classification Report:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       923\n",
      "        spam       0.96      0.99      0.97       370\n",
      "\n",
      "    accuracy                           0.98      1293\n",
      "   macro avg       0.98      0.99      0.98      1293\n",
      "weighted avg       0.98      0.98      0.98      1293\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span>**Overall Accuracy:**</span><br><span style='color:red'>**98.453%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataframe:\n",
    "df2 = load_df(\"dataset/second.csv\")[['text','label']]\n",
    "# Clean it:\n",
    "df2 = clean_dataset(df2, \"text\", \"label\")\n",
    "# Check the distribution:\n",
    "df2[\"label\"].value_counts()\n",
    "# Train split:\n",
    "X_train2, X_test2, y_train2, y_test2 = train_split(df2, \"text\", \"label\")\n",
    "# Create classifier object:\n",
    "predictions2 = create_clf(X_train2, X_test2, y_train2, y_test2)\n",
    "# Save result:\n",
    "accuracy[\"second\"] = accuracy_score(y_test2, predictions2) * 100\n",
    "# Check result\n",
    "print_results(y_test2, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17017971",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d38a5",
   "metadata": {},
   "source": [
    "### Overall Accuracy\n",
    "\n",
    "By choosing the same random_state for all training processes above, we are sure that we get the same results every time we run the codes with the same datasets and algorithms. With that being said, we can confidently talk about the printed numbers above.\n",
    "\n",
    "The overall accuracy for each model is pretty good as each were **above 98%** in accuracy. But that does not mean we have a good trained model. Actually we could have three over-trained models which are good at their own datasets. So how could we test the accuracy of a model outside of its dataset? Simply we can test the test_y of each model with predictions of another classifier object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c2dd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Model Overall Accuracy:\n",
      "first  : 98.533\n",
      "second : 98.453\n"
     ]
    }
   ],
   "source": [
    "# Print each model's overall accuracy:\n",
    "print(\"Individual Model Overall Accuracy:\")\n",
    "for model in accuracy:\n",
    "    print(f\"{model:7}: {accuracy[model]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11176f49",
   "metadata": {},
   "source": [
    "### Models Against Each Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "202908a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_y        predictions      accuracy score     \n",
      "\n",
      "first           first           98.533\n",
      "first           second          63.200\n",
      "second          first           64.267\n",
      "second          second          98.267\n"
     ]
    }
   ],
   "source": [
    "# Find minimum dataset records (in order to be able to calculate predictions):\n",
    "min_df_len = min(y_test.size, y_test2.size)\n",
    "# Init models:\n",
    "models_info = [\n",
    "    (\"first\", y_test[:min_df_len], predictions[:min_df_len]),\n",
    "    (\"second\", y_test2[:min_df_len], predictions2[:min_df_len]),\n",
    "]\n",
    "# Check predictions against each test_y of every model:\n",
    "new_accuracies = []\n",
    "for model in models_info:\n",
    "    new_accuracy_sublist = []\n",
    "    for model2 in models_info:\n",
    "        score = accuracy_score(model[1], model2[2]) * 100\n",
    "        new_accuracy_sublist.append((model, model2, score))\n",
    "    new_accuracies.append(new_accuracy_sublist)\n",
    "# Print the accuracies:\n",
    "print(f\"test_y{'':7}\", f\"predictions{'':5}\", f\"accuracy score{'':5}\", end=\"\\n\\n\")\n",
    "for row in new_accuracies:\n",
    "    for t in row:\n",
    "        print(f\"{t[0][0]:<15} {t[1][0]:15}\", f\"{t[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2848746",
   "metadata": {},
   "source": [
    "As we can see the accuracy scores have dropped even **down to 65%**. This could be a good indicator of several reasons:\n",
    "* a bigger dataset is needed to train our models to gain a stronger model\n",
    "* the models are over-trained\n",
    "* the datasets are not cleaned properly\n",
    "\n",
    "or probably other factors that we are not be aware of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f9be5",
   "metadata": {},
   "source": [
    "### Email vs SMS Spam Detection\n",
    "\n",
    "The first dataset we used is a spam/ham labeled for email texts. The third one is for SMS texts. Now there is a good chance to detect if our email related model can detect SMS related ham/spams and vice-versa. We just need to take a look at first and third models accuracy scores to figure it out:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a8bf8",
   "metadata": {},
   "source": [
    "### Third Dataset Process\n",
    "\n",
    "The process is the same as it was for first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b63d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Confusion Matrix:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1210    1]\n",
      " [  21  161]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Classification Report:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1211\n",
      "        spam       0.99      0.88      0.94       182\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.99      0.94      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span>**Overall Accuracy:**</span><br><span style='color:red'>**98.421%**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataframe:\n",
    "df3 = load_df(\"dataset/third.csv\")[['text','label']]\n",
    "# Clean it:\n",
    "df3 = clean_dataset(df3, \"text\", \"label\")\n",
    "# Check the distribution:\n",
    "df3[\"label\"].value_counts()\n",
    "# Train split:\n",
    "X_train3, X_test3, y_train3, y_test3 = train_split(df3, \"text\", \"label\")\n",
    "# Create classifier object:\n",
    "predictions3 = create_clf(X_train3, X_test3, y_train3, y_test3)\n",
    "# Save result:\n",
    "accuracy[\"third\"] = accuracy_score(y_test3, predictions3) * 100\n",
    "# Check result\n",
    "print_results(y_test3, predictions3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e97fc",
   "metadata": {},
   "source": [
    "### Two Models Against Each Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3ff8992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_y        predictions      accuracy score     \n",
      "\n",
      "first           third           75.200\n",
      "third           first           75.067\n"
     ]
    }
   ],
   "source": [
    "# Find minimum dataset records (in order to be able to calculate predictions):\n",
    "min_df_len = min(y_test.size, y_test3.size)\n",
    "# Init models:\n",
    "models_info = [\n",
    "    (\"first\", y_test[:min_df_len], predictions[:min_df_len]),\n",
    "    (\"third\", y_test3[:min_df_len], predictions3[:min_df_len])\n",
    "]\n",
    "# Check predictions against each test_y of every model:\n",
    "new_accuracies = []\n",
    "for model in models_info:\n",
    "    new_accuracy_sublist = []\n",
    "    for model2 in models_info:\n",
    "        score = accuracy_score(model[1], model2[2]) * 100\n",
    "        new_accuracy_sublist.append((model, model2, score))\n",
    "    new_accuracies.append(new_accuracy_sublist)\n",
    "# Print accuracy scores:\n",
    "print(f\"test_y{'':7}\", f\"predictions{'':5}\", f\"accuracy score{'':5}\", end=\"\\n\\n\")\n",
    "for row in new_accuracies:\n",
    "    for t in row:\n",
    "        if ((t[0][0] == \"first\" or t[0][0] == \"third\") and\n",
    "            (t[1][0] == \"first\" or t[1][0] == \"third\") and\n",
    "            (t[0][0] == \"first\" or t[1][0] == \"first\") and\n",
    "            (t[0][0] == \"third\" or t[1][0] == \"third\")\n",
    "        ):\n",
    "            print(f\"{t[0][0]:<15} {t[1][0]:15}\", f\"{t[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62e435",
   "metadata": {},
   "source": [
    "It's an interesting result! We can see that **by training email spam detection model, we could detect SMS spams, too! And vice-versa.** Some minor difference in accuracy scores, but it's really close. We could guess that spams surely have some similarities in emails and SMS texts. Figuring out these similiarities is another project to work on and it's about Topic Modeling subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b852bf",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Inspired by:\n",
    "\n",
    "- A great tutorial from Udemy: [NLP - Natural Language Processing with Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python/)\n",
    "\n",
    "### Documents:\n",
    "\n",
    "- Spacy Documents: [Spacy Website](https://spacy.io)\n",
    "- Scikit-Learn Documents: [Scikit-Learn Website](https://scikit-learn.org)\n",
    "\n",
    "### Datasets:\n",
    "\n",
    "- Spam or Not Spam Dataset (first): https://www.kaggle.com/ozlerhakan/spam-or-not-spam-dataset\n",
    "- Spam Mails Dataset (second): https://www.kaggle.com/venky73/spam-mails-dataset\n",
    "- Spam Text Message Classification (third): https://www.kaggle.com/team-ai/spam-text-message-classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
